import os  # –ò–º–ø–æ—Ä—Ç –º–æ–¥—É–ª—è 'os' –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π
import yaml  # –ò–º–ø–æ—Ä—Ç –º–æ–¥—É–ª—è 'yaml' –¥–ª—è —á—Ç–µ–Ω–∏—è YAML-—Ñ–∞–π–ª–æ–≤
import openai  # –ò–º–ø–æ—Ä—Ç –º–æ–¥—É–ª—è 'openai' –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å API OpenAI
import telebot  # –ò–º–ø–æ—Ä—Ç –º–æ–¥—É–ª—è 'telebot' –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è Telegram-–±–æ—Ç–∞
from dotenv import load_dotenv  # –ò–º–ø–æ—Ä—Ç —Ñ—É–Ω–∫—Ü–∏–∏ 'load_dotenv' –∏–∑ –º–æ–¥—É–ª—è 'dotenv' –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å—Ä–µ–¥—ã
from logging_config import logger  # –ò–º–ø–æ—Ä—Ç 'logger' –∏–∑ –º–æ–¥—É–ª—è 'logging_config' –¥–ª—è –≤–µ–¥–µ–Ω–∏—è –∂—É—Ä–Ω–∞–ª–∞
from tokens_count import count_tokens  # –ò–º–ø–æ—Ä—Ç —Ñ—É–Ω–∫—Ü–∏–∏ 'count_tokens' –∏–∑ –º–æ–¥—É–ª—è 'tokens_count' –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤
import tiktoken  # –ò–º–ø–æ—Ä—Ç –º–æ–¥—É–ª—è 'tiktoken' –¥–ª—è –æ–ø–µ—Ä–∞—Ü–∏–π —Å –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º

load_dotenv()  # –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å—Ä–µ–¥—ã –∏–∑ —Ñ–∞–π–ª–∞ .env
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")  # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞ –±–æ—Ç–∞ Telegram –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å—Ä–µ–¥—ã
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") # –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–ª—é—á–∞ API OpenAI –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å—Ä–µ–¥—ã


openai.api_key = OPENAI_API_KEY   # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenAI —Å –ø–æ–º–æ—â—å—é –∫–ª—é—á–∞ API


bot = telebot.TeleBot(TELEGRAM_BOT_TOKEN)  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Telegram-–±–æ—Ç–∞ —Å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º —Ç–æ–∫–µ–Ω–æ–º


# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏(encoding) –¥–ª—è –º–æ–¥–µ–ª–∏ GPT
encoding = tiktoken.encoding_for_model("gpt-3.5-turbo-0125") 


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —á—Ç–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–∞ data.yaml
def read_data_from_file(filename):
    with open(filename, 'r', encoding='utf-8') as file:  # –û—Ç–∫—Ä—ã—Ç–∏–µ —Ñ–∞–π–ª–∞ YAML –≤ —Ä–µ–∂–∏–º–µ —á—Ç–µ–Ω–∏—è
        data = yaml.safe_load(file)  # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–∞ YAML
    return data   # –í–æ–∑–≤—Ä–∞—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º OpenAI —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
def generate_openai_response(query, university_data):
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
    max_tokens_per_response = 200  
    
    # –≠—Ç–∞ —á–∞—Å—Ç—å –∫–æ–¥–∞ –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –±–æ–ª–µ–µ –º–µ–ª–∫–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã, –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—è, 
    # —á—Ç–æ –∫–∞–∂–¥—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –æ—Ç–≤–µ—Ç, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º max_tokens_per_response.
    query_chunks = []  
    current_chunk = "" 
    for token in query.split():  
        if len(current_chunk) + len(token) < max_tokens_per_response:  
            current_chunk += token + " "  
        else:
            query_chunks.append(current_chunk.strip())  
            current_chunk = token + " " 
    query_chunks.append(current_chunk.strip()) 
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞
    partial_responses = [] 
    for chunk in query_chunks:  
        prompt = f"{university_data}\n\nUser query: {chunk}"  # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –¥–ª—è OpenAI –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞ –∏ —Ç–µ–∫—É—â–µ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞
        response = openai.ChatCompletion.create(  # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º OpenAI Chat Completion API
            model="gpt-3.5-turbo-0125",   # –£–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ GPT
            messages=[  # –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                # —É–∫–∞–∑—ã–≤–∞—é —Ä–æ–ª—å –º–æ–µ–≥–æ –±–æ—Ç–∞.
                {"role": "system", "content": "You are a helpful assistant for answering RUDN university library-related questions only on the language of the user. You responses are short by containing only the required information."},
                {"role": "user", "content": prompt}, 
            ],
            temperature=0,  # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã –Ω–∞ 0 –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞
            max_tokens=200,  # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
            top_p=1,  # –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –±–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É.
            frequency_penalty=0,  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ —á–∞—Å—Ç–æ—Ç–µ –Ω–∞ 0 –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ (Telling the model not to penalize the frequency of tokens in the response.)
            presence_penalty=0  # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —à—Ç—Ä–∞—Ñ–∞ –Ω–∞–ª–∏—á–∏—è –Ω–∞ 0 –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ ( the model has more freedom to include a wider range of tokens in the generated text, even if they may not be directly related to the user's query or the context provided. )
        )
        partial_response = response.choices[0].message['content'].strip()
    if partial_response:  # Check if the partial response is not empty
        partial_responses.append(partial_response)

#  –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —á–∞—Å—Ç–∏—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏ –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
    complete_response = '\n\n'.join(partial_responses)  

# Remove empty details {'answer': ['']} from the response
    complete_response = complete_response.replace("{'answer': ['']}", "")
    return complete_response  

# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start
@bot.message_handler(commands=['start'])
def start_message(message):
    user_name = message.from_user.first_name   # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–º–µ–Ω–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    welcome_message = f"{user_name}, –¥–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å!üëã\n–Ø Telegram-–±–æ—Ç, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –æ–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –æ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞ –†–£–î–ù, –∏—Å–ø–æ–ª—å–∑—É—è OpenAI.\n–ù–µ —Å—Ç–µ—Å–Ω—è–π—Ç–µ—Å—å –∑–∞–¥–∞–≤–∞—Ç—å –º–Ω–µ —á—Ç–æ —É–≥–æ–¥–Ω–æ.üòä"  # Creating a welcome message
    bot.send_message(message.chat.id, welcome_message)  # –û—Ç–ø—Ä–∞–≤–∫–∞ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é

# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–æ–æ–±—â–µ–Ω–∏–π —Å —Ç–µ–∫—Å—Ç–æ–º
@bot.message_handler(func=lambda message:True)
def handle_message(message):
    user_input = message.text  # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è, –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º

    # –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–∞ data.yaml
    university_data = read_data_from_file("data.yaml")  

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º OpenAI —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    openai_response = generate_openai_response(user_input, university_data)  

    # –†–∞—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
    generated_tokens = len(logger.encoding.encode(openai_response))  
   
    logger.log_message('User', user_input) 
    
    
    logger.log_message('Bot', openai_response) 

    # –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
    logger.log_generated_tokens(generated_tokens)  

     # –û—Ç–ø—Ä–∞–≤–∫–∞ –æ—Ç–≤–µ—Ç–∞, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ OpenAI, –æ–±—Ä–∞—Ç–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
    bot.send_message(message.chat.id, openai_response)  
bot.polling() 
